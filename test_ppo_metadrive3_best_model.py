"""
Test script for ppo_metadrive_best_model.pth generated by ppo_metadrive3.py

This script is specifically designed to test the best model saved by ppo_metadrive3.py,
which includes VAE, FiLM layers, and other advanced components.

Usage:
    python test_ppo_metadrive3_best_model.py
    python test_ppo_metadrive3_best_model.py --episodes 10 --no-render
    python test_ppo_metadrive3_best_model.py --video --episodes 1
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import numpy.typing as npt
import gymnasium as gym
import matplotlib.pyplot as plt
import time
import os
import argparse
from typing import List, Tuple, Optional, Dict, Any
from dataclasses import dataclass
import logging

# Import from ppo_metadrive3.py
from ppo_metadrive3 import (
    PPOConfig, Actor, Critic, ImageVAE_CNN840Micro_LinearDec, ZValue,
    NNPolicy, obs_batch_to_tensor, deviceof, collect_trajectory
)

# Register MetaDrive environment
from metadrive.envs.top_down_env import TopDownMetaDrive
gym.register(id="MetaDrive-topdown", entry_point=TopDownMetaDrive, kwargs=dict(config={}))

# Disable logging from metadrive
logging.getLogger("metadrive.envs.base_env").setLevel(logging.WARNING)


class PPO3ModelTester:
    """Test class for trained PPO3 models with VAE and advanced architecture"""
    
    def __init__(self, model_path: str = "ppo_metadrive_best_model.pth", config: Optional[PPOConfig] = None):
        """
        Initialize the tester with a trained PPO3 model
        
        Args:
            model_path: Path to the saved model (.pth file)
            config: PPO configuration (optional, will be loaded from model if not provided)
        """
        self.model_path = model_path
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Load model
        self.load_model(model_path, config)
        
        # Initialize policy
        self.policy = NNPolicy(self.actor, vae=self.vae_guide, use_recon_for_policy=True)
        
        print(f"Model loaded successfully from {model_path}")
        print(f"Using device: {self.device}")
        print(f"Model architecture: PPO3 with VAE and FiLM layers")

    def load_model(self, model_path: str, config: Optional[PPOConfig] = None):
        """Load the trained PPO3 model with all components"""
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model file not found: {model_path}")
        
        checkpoint = torch.load(model_path, map_location=self.device)
        
        # Load configuration
        if config is None:
            self.config = checkpoint.get('config', PPOConfig())
        else:
            self.config = config
        
        # Initialize networks with correct architecture
        self.actor = Actor(input_channels=5, z_dim=40).to(self.device)
        self.critic = Critic(input_channels=5, z_dim=40).to(self.device)
        self.vae_guide = ImageVAE_CNN840Micro_LinearDec(recon_activation=None).to(self.device)
        self.z_value = ZValue(z_dim=40).to(self.device)
        
        # Load state dicts
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.vae_guide.load_state_dict(checkpoint['vae_state_dict'])
        
        # Set to evaluation mode
        self.actor.eval()
        self.critic.eval()
        self.vae_guide.eval()
        self.z_value.eval()
        
        print("PPO3 model loaded successfully!")
        print(f"  - Actor: {sum(p.numel() for p in self.actor.parameters())} parameters")
        print(f"  - Critic: {sum(p.numel() for p in self.critic.parameters())} parameters")
        print(f"  - VAE: {sum(p.numel() for p in self.vae_guide.parameters())} parameters")

    def create_env(self, render: bool = True, horizon: int = 500, num_scenarios: int = 100) -> gym.Env:
        """Create MetaDrive environment for testing"""
        return gym.make(
            "MetaDrive-topdown",
            config={
                "use_render": render,
                "horizon": horizon,
                "num_scenarios": num_scenarios
            }
        )

    def test_single_episode(self, render: bool = True, save_video: bool = False, 
                          episode_id: int = 0, verbose: bool = True) -> Dict[str, Any]:
        """
        Test the model on a single episode with detailed metrics
        
        Args:
            render: Whether to render the environment
            save_video: Whether to save video frames
            episode_id: Episode identifier for saving
            verbose: Whether to print detailed progress
            
        Returns:
            Dictionary with comprehensive episode statistics
        """
        env = self.create_env(render=render, horizon=500)
        
        obs, info = env.reset()
        done = False
        total_reward = 0
        step_count = 0
        frames = [] if save_video else None
        
        # Detailed metrics
        action_history = []
        reward_history = []
        vae_reconstruction_losses = []
        critic_values = []
        
        if verbose:
            print(f"Starting episode {episode_id}...")
        
        while not done:
            # Get action from policy (includes VAE processing)
            action = self.policy(obs)
            action_history.append(action)
            
            # Take step
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            
            total_reward += reward
            reward_history.append(reward)
            step_count += 1
            
            # Compute additional metrics
            with torch.no_grad():
                obs_tensor = obs_batch_to_tensor([obs], self.device)
                _, mu, logvar, z = self.vae_guide(obs_tensor, return_latent=True)
                z_reshaped = z.view(z.size(0), 20, 2)
                
                # VAE reconstruction loss
                obs_flat = obs_tensor.reshape(obs_tensor.size(0), -1)
                recon_loss = F.mse_loss(mu, obs_flat[:, :40])
                vae_reconstruction_losses.append(recon_loss.item())
                
                # Critic value
                value = self.critic(obs_tensor, z_reshaped)
                critic_values.append(value.item())
            
            # Save frame if requested
            if save_video and render:
                frames.append(env.render())
            
            # Print progress every 50 steps
            if verbose and step_count % 50 == 0:
                print(f"  Step {step_count}, Reward: {total_reward:.2f}, "
                      f"Critic Value: {critic_values[-1]:.3f}")
        
        env.close()
        
        episode_stats = {
            'total_reward': total_reward,
            'step_count': step_count,
            'success': not truncated,
            'frames': frames,
            'action_history': action_history,
            'reward_history': reward_history,
            'vae_reconstruction_losses': vae_reconstruction_losses,
            'critic_values': critic_values,
            'avg_critic_value': np.mean(critic_values) if critic_values else 0,
            'avg_vae_loss': np.mean(vae_reconstruction_losses) if vae_reconstruction_losses else 0,
            'action_std': np.std([a[0] for a in action_history]) if action_history else 0,  # throttle std
            'steering_std': np.std([a[1] for a in action_history]) if action_history else 0,  # steering std
        }
        
        if verbose:
            print(f"Episode {episode_id} completed!")
            print(f"  Total reward: {total_reward:.2f}")
            print(f"  Steps: {step_count}")
            print(f"  Success: {episode_stats['success']}")
            print(f"  Avg Critic Value: {episode_stats['avg_critic_value']:.3f}")
            print(f"  Avg VAE Loss: {episode_stats['avg_vae_loss']:.4f}")
            print(f"  Action Std (throttle/steering): {episode_stats['action_std']:.3f}/{episode_stats['steering_std']:.3f}")
        
        return episode_stats

    def test_multiple_episodes(self, num_episodes: int = 5, render: bool = True, 
                              verbose: bool = True) -> List[Dict[str, Any]]:
        """
        Test the model on multiple episodes with comprehensive analysis
        
        Args:
            num_episodes: Number of episodes to test
            render: Whether to render the environment
            verbose: Whether to print detailed progress
            
        Returns:
            List of episode statistics
        """
        if verbose:
            print(f"Testing PPO3 model on {num_episodes} episodes...")
        
        all_episodes = []
        total_rewards = []
        
        for i in range(num_episodes):
            if verbose:
                print(f"\n--- Episode {i+1}/{num_episodes} ---")
            
            episode_stats = self.test_single_episode(
                render=render, 
                episode_id=i+1, 
                verbose=verbose
            )
            all_episodes.append(episode_stats)
            total_rewards.append(episode_stats['total_reward'])
            
            # Small delay between episodes
            time.sleep(0.5)
        
        # Print comprehensive summary statistics
        if verbose:
            self._print_summary_statistics(all_episodes, total_rewards)
        
        return all_episodes

    def _print_summary_statistics(self, all_episodes: List[Dict[str, Any]], total_rewards: List[float]):
        """Print comprehensive summary statistics"""
        print(f"\n{'='*60}")
        print(f"PPO3 MODEL TEST RESULTS SUMMARY")
        print(f"{'='*60}")
        print(f"Number of episodes: {len(all_episodes)}")
        print(f"Average reward: {np.mean(total_rewards):.2f} ¬± {np.std(total_rewards):.2f}")
        print(f"Best reward: {np.max(total_rewards):.2f}")
        print(f"Worst reward: {np.min(total_rewards):.2f}")
        print(f"Success rate: {np.mean([ep['success'] for ep in all_episodes]):.2%}")
        
        # Additional metrics
        avg_critic_values = [ep['avg_critic_value'] for ep in all_episodes]
        avg_vae_losses = [ep['avg_vae_loss'] for ep in all_episodes]
        action_stds = [ep['action_std'] for ep in all_episodes]
        steering_stds = [ep['steering_std'] for ep in all_episodes]
        
        print(f"\nDetailed Metrics:")
        print(f"  Average Critic Value: {np.mean(avg_critic_values):.3f} ¬± {np.std(avg_critic_values):.3f}")
        print(f"  Average VAE Loss: {np.mean(avg_vae_losses):.4f} ¬± {np.std(avg_vae_losses):.4f}")
        print(f"  Throttle Action Std: {np.mean(action_stds):.3f} ¬± {np.std(action_stds):.3f}")
        print(f"  Steering Action Std: {np.mean(steering_stds):.3f} ¬± {np.std(steering_stds):.3f}")
        
        # Performance assessment
        avg_reward = np.mean(total_rewards)
        if avg_reward > 100:
            print(f"\nüéâ EXCELLENT performance! Model is highly trained.")
        elif avg_reward > 50:
            print(f"\nüëç GOOD performance! Model shows solid driving skills.")
        elif avg_reward > 20:
            print(f"\nüìà FAIR performance! Model has basic driving ability.")
        else:
            print(f"\n‚ö†Ô∏è  POOR performance! Model needs more training.")

    def save_video(self, frames: List, filename: str = "ppo3_test_video.mp4"):
        """Save video frames to file"""
        if not frames:
            print("No frames to save")
            return
        
        try:
            import cv2
            
            # Get frame dimensions
            height, width, channels = frames[0].shape
            
            # Create video writer
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(filename, fourcc, 20.0, (width, height))
            
            # Write frames
            for frame in frames:
                # Convert RGB to BGR for OpenCV
                frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                out.write(frame_bgr)
            
            out.release()
            print(f"Video saved as {filename}")
            
        except ImportError:
            print("OpenCV not available. Saving frames as images instead...")
            self.save_frames_as_images(frames, filename.replace('.mp4', ''))

    def save_frames_as_images(self, frames: List, prefix: str = "ppo3_frame"):
        """Save frames as individual images"""
        for i, frame in enumerate(frames):
            plt.figure(figsize=(10, 10))
            plt.imshow(frame)
            plt.axis('off')
            plt.title(f"PPO3 Frame {i}")
            plt.savefig(f"{prefix}_{i:04d}.png", bbox_inches='tight', dpi=100)
            plt.close()
        
        print(f"Saved {len(frames)} frames as images with prefix '{prefix}'")

    def plot_comprehensive_performance(self, episode_stats: List[Dict[str, Any]], 
                                     save_plot: bool = True):
        """Plot comprehensive performance metrics for PPO3 model"""
        rewards = [ep['total_reward'] for ep in episode_stats]
        steps = [ep['step_count'] for ep in episode_stats]
        successes = [ep['success'] for ep in episode_stats]
        critic_values = [ep['avg_critic_value'] for ep in episode_stats]
        vae_losses = [ep['avg_vae_loss'] for ep in episode_stats]
        action_stds = [ep['action_std'] for ep in episode_stats]
        steering_stds = [ep['steering_std'] for ep in episode_stats]
        
        fig, axes = plt.subplots(3, 3, figsize=(18, 15))
        fig.suptitle('PPO3 Model Performance Analysis', fontsize=16, fontweight='bold')
        
        # Reward plot
        axes[0, 0].plot(rewards, 'b-o', linewidth=2, markersize=6)
        axes[0, 0].set_title('Episode Rewards')
        axes[0, 0].set_xlabel('Episode')
        axes[0, 0].set_ylabel('Total Reward')
        axes[0, 0].grid(True, alpha=0.3)
        
        # Steps plot
        axes[0, 1].plot(steps, 'g-o', linewidth=2, markersize=6)
        axes[0, 1].set_title('Episode Length')
        axes[0, 1].set_xlabel('Episode')
        axes[0, 1].set_ylabel('Steps')
        axes[0, 1].grid(True, alpha=0.3)
        
        # Success rate
        success_rate = np.cumsum(successes) / np.arange(1, len(successes) + 1)
        axes[0, 2].plot(success_rate, 'r-o', linewidth=2, markersize=6)
        axes[0, 2].set_title('Cumulative Success Rate')
        axes[0, 2].set_xlabel('Episode')
        axes[0, 2].set_ylabel('Success Rate')
        axes[0, 2].set_ylim(0, 1)
        axes[0, 2].grid(True, alpha=0.3)
        
        # Critic values
        axes[1, 0].plot(critic_values, 'purple', linewidth=2, marker='o', markersize=6)
        axes[1, 0].set_title('Average Critic Values')
        axes[1, 0].set_xlabel('Episode')
        axes[1, 0].set_ylabel('Critic Value')
        axes[1, 0].grid(True, alpha=0.3)
        
        # VAE losses
        axes[1, 1].plot(vae_losses, 'orange', linewidth=2, marker='o', markersize=6)
        axes[1, 1].set_title('Average VAE Reconstruction Losses')
        axes[1, 1].set_xlabel('Episode')
        axes[1, 1].set_ylabel('VAE Loss')
        axes[1, 1].grid(True, alpha=0.3)
        
        # Action standard deviations
        axes[1, 2].plot(action_stds, 'brown', linewidth=2, marker='o', markersize=6, label='Throttle')
        axes[1, 2].plot(steering_stds, 'pink', linewidth=2, marker='s', markersize=6, label='Steering')
        axes[1, 2].set_title('Action Standard Deviations')
        axes[1, 2].set_xlabel('Episode')
        axes[1, 2].set_ylabel('Action Std')
        axes[1, 2].legend()
        axes[1, 2].grid(True, alpha=0.3)
        
        # Reward distribution
        axes[2, 0].hist(rewards, bins=min(10, len(rewards)), alpha=0.7, color='purple', edgecolor='black')
        axes[2, 0].set_title('Reward Distribution')
        axes[2, 0].set_xlabel('Total Reward')
        axes[2, 0].set_ylabel('Frequency')
        axes[2, 0].grid(True, alpha=0.3)
        
        # Step distribution
        axes[2, 1].hist(steps, bins=min(10, len(steps)), alpha=0.7, color='green', edgecolor='black')
        axes[2, 1].set_title('Step Distribution')
        axes[2, 1].set_xlabel('Steps')
        axes[2, 1].set_ylabel('Frequency')
        axes[2, 1].grid(True, alpha=0.3)
        
        # Performance summary
        axes[2, 2].text(0.1, 0.8, f'Episodes: {len(episode_stats)}', transform=axes[2, 2].transAxes, fontsize=12)
        axes[2, 2].text(0.1, 0.7, f'Avg Reward: {np.mean(rewards):.2f}', transform=axes[2, 2].transAxes, fontsize=12)
        axes[2, 2].text(0.1, 0.6, f'Best Reward: {np.max(rewards):.2f}', transform=axes[2, 2].transAxes, fontsize=12)
        axes[2, 2].text(0.1, 0.5, f'Success Rate: {np.mean(successes):.1%}', transform=axes[2, 2].transAxes, fontsize=12)
        axes[2, 2].text(0.1, 0.4, f'Avg Steps: {np.mean(steps):.1f}', transform=axes[2, 2].transAxes, fontsize=12)
        axes[2, 2].text(0.1, 0.3, f'Avg Critic: {np.mean(critic_values):.3f}', transform=axes[2, 2].transAxes, fontsize=12)
        axes[2, 2].text(0.1, 0.2, f'Avg VAE Loss: {np.mean(vae_losses):.4f}', transform=axes[2, 2].transAxes, fontsize=12)
        axes[2, 2].set_title('Performance Summary')
        axes[2, 2].axis('off')
        
        plt.tight_layout()
        
        if save_plot:
            plt.savefig('ppo3_test_performance.png', dpi=300, bbox_inches='tight')
            print("Comprehensive performance plot saved as 'ppo3_test_performance.png'")
        
        plt.show()

    def interactive_test(self):
        """Interactive testing mode for PPO3 model"""
        print("=== Interactive PPO3 Model Testing ===")
        print("Commands:")
        print("  'test' - Test single episode")
        print("  'multi' - Test multiple episodes")
        print("  'video' - Test with video recording")
        print("  'analyze' - Detailed analysis of single episode")
        print("  'quit' - Exit")
        
        while True:
            command = input("\nEnter command: ").strip().lower()
            
            if command == 'quit':
                break
            elif command == 'test':
                self.test_single_episode(render=True, verbose=True)
            elif command == 'multi':
                try:
                    num_episodes = int(input("Number of episodes (default 5): ") or "5")
                    episodes = self.test_multiple_episodes(num_episodes, render=True, verbose=True)
                    self.plot_comprehensive_performance(episodes)
                except ValueError:
                    print("Invalid number, using default 5 episodes")
                    episodes = self.test_multiple_episodes(5, render=True, verbose=True)
                    self.plot_comprehensive_performance(episodes)
            elif command == 'video':
                print("Recording video...")
                episode_stats = self.test_single_episode(render=True, save_video=True, verbose=True)
                if episode_stats['frames']:
                    self.save_video(episode_stats['frames'])
            elif command == 'analyze':
                print("Running detailed analysis...")
                episode_stats = self.test_single_episode(render=True, verbose=True)
                print(f"\nDetailed Analysis:")
                print(f"  Action range: throttle [{min(a[0] for a in episode_stats['action_history']):.3f}, {max(a[0] for a in episode_stats['action_history']):.3f}]")
                print(f"  Action range: steering [{min(a[1] for a in episode_stats['action_history']):.3f}, {max(a[1] for a in episode_stats['action_history']):.3f}]")
                print(f"  Reward trend: {episode_stats['reward_history'][:5]}...{episode_stats['reward_history'][-5:]}")
            else:
                print("Unknown command. Available: test, multi, video, analyze, quit")


def main():
    """Main function for testing PPO3 model"""
    parser = argparse.ArgumentParser(description='Test trained PPO3 MetaDrive model')
    parser.add_argument('--model', type=str, default='ppo_metadrive_best_model.pth',
                       help='Path to trained model file')
    parser.add_argument('--episodes', type=int, default=5,
                       help='Number of episodes to test')
    parser.add_argument('--no-render', action='store_true',
                       help='Disable rendering (faster testing)')
    parser.add_argument('--video', action='store_true',
                       help='Record video of first episode')
    parser.add_argument('--interactive', action='store_true',
                       help='Run in interactive mode')
    parser.add_argument('--quiet', action='store_true',
                       help='Reduce output verbosity')
    
    args = parser.parse_args()
    
    # Check if model exists
    if not os.path.exists(args.model):
        print(f"‚ùå Model file not found: {args.model}")
        print("Please ensure you have trained a model using ppo_metadrive3.py")
        return
    
    try:
        # Create tester
        print("üöó Initializing PPO3 Model Tester...")
        tester = PPO3ModelTester(args.model)
        
        if args.interactive:
            # Interactive mode
            tester.interactive_test()
        else:
            # Standard testing
            if args.video:
                # Test with video recording
                print("Testing with video recording...")
                episode_stats = tester.test_single_episode(
                    render=not args.no_render, 
                    save_video=True,
                    verbose=not args.quiet
                )
                if episode_stats['frames']:
                    tester.save_video(episode_stats['frames'])
            else:
                # Test multiple episodes
                episode_stats = tester.test_multiple_episodes(
                    num_episodes=args.episodes,
                    render=not args.no_render,
                    verbose=not args.quiet
                )
                
                # Plot comprehensive performance
                if not args.quiet:
                    tester.plot_comprehensive_performance(episode_stats)
    
    except Exception as e:
        print(f"‚ùå Error during testing: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
